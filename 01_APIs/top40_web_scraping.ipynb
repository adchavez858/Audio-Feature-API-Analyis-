{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbe7c2b",
   "metadata": {},
   "source": [
    "Use this notebook to scrape weekly top 40 charts\n",
    "- USA Top 40 singles\n",
    "- https://top40-charts.com/chart.php?cid=27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eec7738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import timeit\n",
    "from IPython.display import clear_output\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "55b8da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a time series for charts\n",
    "# note: chart weekday changes after 03/27/2000, need to concatenate two time series into one\n",
    "ts1 = pd.date_range(start='1997-07-07', end='2000-03-27', freq='W-MON', inclusive='both', normalize=True).to_series()\n",
    "ts2 = pd.date_range(start='2000-04-01', end='2022-10-15', freq='W-SAT', inclusive='both', normalize=True).to_series()\n",
    "\n",
    "ts = pd.concat([ts1, ts2])\n",
    "# create a base URL for all charts\n",
    "chart_url = 'https://top40-charts.com/chart.php?cid=27&date='\n",
    "\n",
    "# concatenate URLs and dates to get full URLs for each weekly chart\n",
    "urls = [] # empty list to hold full URLs\n",
    "for w, date in enumerate(ts):\n",
    "    \n",
    "    ts_date = re.search('(\\d{4}-\\d{2}-\\d{2})(?:\\s)', str(ts[w])).group(1)\n",
    "    url_date = chart_url + ts_date\n",
    "    urls.append(url_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "41f284f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape chart data \n",
    "# input: a list of weekly chart URLs to scrape\n",
    "\n",
    "def get_top40_charts(weekly_charts):\n",
    "    \n",
    "    # start timer\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    # set up an empty dictionary to hold the final dataset\n",
    "    top40_data = []\n",
    "\n",
    "    # counter to keep track of total songs scraped\n",
    "    total_songs = 0\n",
    "\n",
    "    # set up the browser\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "    for w, chart in enumerate(weekly_charts):\n",
    "        \n",
    "        # get the chart's date\n",
    "        url_date = re.search('date=(\\d{4}-\\d{2}-\\d{2})', chart).group(1)\n",
    "\n",
    "        # navigate to website\n",
    "        browser.visit(chart)\n",
    "        \n",
    "        # parse the html\n",
    "        chart_html = browser.html\n",
    "        chart_soup = soup(chart_html, 'html.parser')\n",
    "        \n",
    "        # list of html data for all chart positions\n",
    "        chart_rows = chart_soup.find_all('tr', class_='latc_song')\n",
    "        \n",
    "        # empty lists to hold chart data\n",
    "        position = []\n",
    "        artist = []\n",
    "        title = []\n",
    "        \n",
    "        for row in chart_rows:\n",
    "            \n",
    "            # position is saved as 'chid' attribute\n",
    "            position.append(row['chid'])\n",
    "            \n",
    "            # extract song titles using regex\n",
    "            row_a = row.find_all('a')\n",
    "            row_title = re.search('(?:;\" title=\"View song details\">)(.+)(?:</a>, <a href)', str(row_a)).group(1)\n",
    "            title.append(row_title)\n",
    "            \n",
    "            # extract artist names using regex\n",
    "            # depending on the date, artist name is located in different places,\n",
    "            # so using a try/except blocks to use whichever works\n",
    "            \n",
    "            # empty list to hold multiple artist name searches\n",
    "            artist_tries = []\n",
    "            \n",
    "            try:\n",
    "                row_artist_1 = re.search('(?:>)(.+)(?:</a>)', str(row_a[2])).group(1)\n",
    "                artist_tries.append(row_artist_1)            \n",
    "            except Exception as e:\n",
    "                print(f'{url_date} regex try1 error: {e}')\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                row_artist_2 = re.search('(?:>)(.+)(?:</a>)', str(row_a[1])).group(1)\n",
    "                artist_tries.append(row_artist_2)\n",
    "            except Exception as e:\n",
    "                print(url_date)\n",
    "                print(f'{url_date} regex try2 error: {e}')\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                row_artist_3 = re.search('(?:style=\"text-decoration: none; \">)(.+)(?:</a>, <a chid)', str(row_a)).group(1)\n",
    "                artist_tries.append(row_artist_3)\n",
    "            except Exception as e:\n",
    "                print(url_date)\n",
    "                print(f'{url_date} regex try3 error: {e}')\n",
    "                pass\n",
    "            \n",
    "            # if the artist search captured the song title, remove it from the list\n",
    "            artist_tries.remove(row_title)\n",
    "            \n",
    "            # keep the shortest string search result that worked: \n",
    "            # some searches capture html attributes, resulting in long strings - ignore those\n",
    "            # but this won't work if only one search worked, so use try/except\n",
    "            try:\n",
    "                row_artist = min(artist_tries, key=len)\n",
    "            except:\n",
    "                row_artist = artist_tries\n",
    "            \n",
    "            # save the best artist search to the list\n",
    "            artist.append(row_artist)\n",
    "        \n",
    "        \n",
    "        # create datetime data\n",
    "        date_list = [url_date] * len(artist)\n",
    "        #date_list = pd.to_datetime(date_list)\n",
    "\n",
    "        # save the week's charts to a dictionary\n",
    "        current_chart = {\n",
    "            'week' : date_list,\n",
    "            'position' : position,\n",
    "            'song' : title,\n",
    "            'artist' : artist\n",
    "        }\n",
    "        \n",
    "        # track how many songs have been scraped\n",
    "        total_songs += len(title)\n",
    "\n",
    "        # save the week's chart \n",
    "        top40_data.append(current_chart)\n",
    "        \n",
    "        \n",
    "        # log the progress\n",
    "        clear_output()\n",
    "        checkpoint = timeit.default_timer()\n",
    "        print(f'Last saved date: {url_date}')\n",
    "        print(f'{(w+1)}/{len(weekly_charts)} charts scraped')\n",
    "        print(f'{(w+1)/len(weekly_charts)*100:.2f}% complete')\n",
    "        print(f'{(checkpoint-start)/60:.2f} minutes elapsed...')\n",
    "        \n",
    "\n",
    "    # save all data to a dataframe\n",
    "    final_df = pd.concat(pd.DataFrame(chart_dict) for chart_dict in top40_data)\n",
    "\n",
    "    # end timer, log results\n",
    "    clear_output()\n",
    "    stop = timeit.default_timer()\n",
    "    print(f'Last saved date: {url_date}')\n",
    "    print(f'{(w+1)/len(weekly_charts)*100:.2f}% complete')\n",
    "    print(f'Total runtime: {(stop-start)/60:.2f} minutes')\n",
    "    print(f\"{total_songs:.0f} total songs and {len(top40_data):.0f} total weeks scraped\")\n",
    "    \n",
    "    # results\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "503dd9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last saved date: 2022-10-15\n",
      "100.00% complete\n",
      "Total runtime: 41.45 minutes\n",
      "52768 total songs and 1320 total weeks scraped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>position</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-04-22</td>\n",
       "      <td>5</td>\n",
       "      <td>I Try</td>\n",
       "      <td>Macy Gray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2002-12-28</td>\n",
       "      <td>20</td>\n",
       "      <td>I'm With You</td>\n",
       "      <td>Avril Lavigne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-04-06</td>\n",
       "      <td>5</td>\n",
       "      <td>3 Am</td>\n",
       "      <td>Matchbox 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1998-01-26</td>\n",
       "      <td>33</td>\n",
       "      <td>No, No, No</td>\n",
       "      <td>Destiny's Child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>19</td>\n",
       "      <td>Despues De La Playa</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2008-05-03</td>\n",
       "      <td>28</td>\n",
       "      <td>Realize</td>\n",
       "      <td>Colbie Caillat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-06-24</td>\n",
       "      <td>3</td>\n",
       "      <td>Be With You</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-03-04</td>\n",
       "      <td>18</td>\n",
       "      <td>24K Magic</td>\n",
       "      <td>Bruno Mars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2006-07-15</td>\n",
       "      <td>15</td>\n",
       "      <td>So What</td>\n",
       "      <td>Field Mob &amp;amp; Ciara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009-06-20</td>\n",
       "      <td>9</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>David Cook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          week position                 song                 artist\n",
       "4   2000-04-22        5                I Try              Macy Gray\n",
       "19  2002-12-28       20         I'm With You          Avril Lavigne\n",
       "4   1998-04-06        5                 3 Am            Matchbox 20\n",
       "32  1998-01-26       33           No, No, No        Destiny's Child\n",
       "18  2022-06-18       19  Despues De La Playa              Bad Bunny\n",
       "27  2008-05-03       28              Realize         Colbie Caillat\n",
       "2   2000-06-24        3          Be With You       Enrique Iglesias\n",
       "17  2017-03-04       18            24K Magic             Bruno Mars\n",
       "14  2006-07-15       15              So What  Field Mob &amp; Ciara\n",
       "8   2009-06-20        9            Permanent             David Cook"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top40_df = get_top40_charts(urls)\n",
    "top40_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "549cc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "top40_df.to_csv(\"../00_data/top40_1997_2022_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258a619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pythondata",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
